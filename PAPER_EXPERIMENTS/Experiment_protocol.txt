MASTER EXPERIMENTAL PROTOCOL
DSpar Paper - Complete Experimental Validation

This document outlines the complete experimental suite for validating
the DSpar paper's theoretical contributions and practical effectiveness.

Author: Mohammad Dindoost
Date: January 2026


# ===========================================================================
# EXPERIMENTAL STRUCTURE
# ===========================================================================


Part I: MECHANISM VALIDATION (Prove causality)
├── Experiment 1.1: Controlled Synthetic Networks
│   Purpose: Prove hub-bridge hypothesis through factorial design
│   File: exp1_1_synthetic_controlled.py
│   Key Question: Does DSpar work if and only if degree heterogeneity AND hub-bridging?
│   Expected Result: Yes - modularity improves only when both conditions met
│
├── Experiment 1.2: Theoretical Predictions Validation
│   Purpose: Test quantitative predictions from Theorems 1-3
│   File: exp1_2_theoretical_predictions.py [dataset]
│   Key Question: Do mathematical predictions match empirical observations?
│   Expected Result: High correlation (r > 0.95) between predicted and observed
│
└── Experiment 1.3: LFR Benchmark Analysis
    Purpose: Explain why standard LFR doesn't show improvements
    File: exp1_3_lfr_analysis.py
    Key Question: What structural property does LFR miss?
    Expected Result: LFR lacks hub-bridging; modified LFR with hub-bridging works

Part II: REAL NETWORK PERFORMANCE (Practical effectiveness)
├── Experiment 2.1: Main Performance Evaluation
│   Purpose: Demonstrate improvements on diverse real networks
│   File: exp2_1_real_network_performance.py [dataset]
│   Key Question: Does DSpar improve Q and NMI on real networks?
│   Expected Result: 3-33% modularity gains at 50% retention
│
└── Experiment 2.2: Weighting vs Sampling
    Purpose: Compare theoretical approach (weighting) vs practical (sampling)
    File: exp_remaining.py 2.2 [dataset]
    Key Question: Does sampling approximate weighting (Theorem 3)?
    Expected Result: Converge asymptotically; sampling faster

Part III: EFFICIENCY ANALYSIS (Scalability)
└── Experiment 3.1: Scalability Validation
    Purpose: Prove O(m) complexity and measure speedup
    File: exp_remaining.py 3.1
    Key Question: Is DSpar truly O(m)? What's the practical speedup?
    Expected Result: O(m) scaling confirmed; 2-3× end-to-end speedup

Part IV: BOUNDARY CONDITIONS (When does it fail?)
├── Experiment 4.1: Property Correlation (Diagnostic Framework)
│   Purpose: Identify which network properties predict success
│   File: exp_remaining.py 4.1
│   Key Question: Can we predict when DSpar works from network properties?
│   Expected Result: δ and hub-bridge correlation are strong predictors
│
└── Experiment 4.2: Failure Cases
    Purpose: Test networks where DSpar should NOT work
    File: exp_remaining.py 4.2
    Key Question: Does DSpar fail predictably on homogeneous networks?
    Expected Result: δ ≈ 0 → no improvement (validates boundary conditions)


# ===========================================================================
# QUICK START GUIDE
# ===========================================================================


PREREQUISITE: Install dependencies

pip install numpy networkx igraph leidenalg scikit-learn pandas scipy matplotlib


STEP 1: Run Mechanism Validation (Most Important)

# Prove causality with controlled experiments
python exp1_1_synthetic_controlled.py

# Validate theoretical predictions on real data
python exp1_2_theoretical_predictions.py cit-HepPh
python exp1_2_theoretical_predictions.py facebook

# Explain LFR failure
python exp1_3_lfr_analysis.py


STEP 2: Real Network Performance

# Main results on multiple networks
python exp2_1_real_network_performance.py cit-HepPh
python exp2_1_real_network_performance.py cit-HepTh
python exp2_1_real_network_performance.py facebook
python exp2_1_real_network_performance.py ca-GrQc
python exp2_1_real_network_performance.py road-CA  # Should fail

# Weighting vs sampling
python exp_remaining.py 2.2 cit-HepPh


STEP 3: Efficiency & Scalability

python exp_remaining.py 3.1


STEP 4: Boundary Conditions

# Diagnostic framework
python exp_remaining.py 4.1

# Explicit failure cases
python exp_remaining.py 4.2



# ===========================================================================
# EXPECTED OUTPUTS & FILES
# ===========================================================================


results/
├── exp1_1_synthetic/
│   ├── raw_results.csv                    # All experimental conditions
│   ├── predicted_vs_observed_ratio.csv    # Theorem 1 validation
│   └── delta_vs_improvement.csv           # Mechanism validation
│
├── exp1_2_theoretical/
│   ├── [dataset]_theoretical_validation.csv     # Full trial data
│   └── [dataset]_plot_data.csv                  # Aggregated for plotting
│
├── exp1_3_lfr/
│   ├── lfr_analysis_results.csv           # Standard vs Modified LFR
│   └── delta_vs_improvement.csv           # Correlation analysis
│
├── exp2_1_real_networks/
│   ├── [dataset]_results.csv              # All retention rates, methods
│   └── [dataset]_summary.json             # Key metrics & properties
│
├── exp2_2_weighting_vs_sampling/
│   └── [dataset]_weighting_vs_sampling.csv
│
├── exp3_1_scalability/
│   └── scalability_results.csv            # Time vs size
│
├── exp4_1_property_correlation/
│   └── property_correlation.csv           # Cross-network analysis
│
└── exp4_2_failure_cases/
    └── failure_cases.csv                  # Boundary condition tests


# ===========================================================================
# KEY METRICS & INTERPRETATIONS
# ===========================================================================


PRIMARY METRICS:

1. Modularity (Q):
   - Original: Baseline quality
   - After sparsification: New quality
   - ΔQ = Q_sparse - Q_original
   - ΔQ% = 100 * ΔQ / Q_original
   - Expected: +3% to +33% at 50% retention

2. NMI (Normalized Mutual Information):
   - Measures agreement with original clustering
   - Range: [0, 1], higher = more consistent
   - Expected: > 0.85 (high agreement)

3. Preservation Ratio:
   - Ratio = (inter-community rate) / (intra-community rate)
   - Ratio < 1: Inter-edges removed faster (DESIRED)
   - Ratio = 1: No preference (like random)
   - Ratio > 1: Intra-edges removed faster (BAD)
   - Expected: 0.7-0.9 for working cases

4. DSpar Separation (δ):
   - δ = μ_intra - μ_inter
   - δ > 0: DSpar should work
   - δ ≈ 0: DSpar won't work
   - Larger δ: Stronger effect

5. Hub-Bridge Correlation:
   - Correlation = E[d_u·d_v | inter] - E[d_u·d_v | intra]
   - Positive: Inter-edges connect hubs
   - ≈ 0: No hub-bridging
   - Expected: > 0 for real networks, ≈ 0 for LFR

SECONDARY METRICS:

6. Connected Components:
   - Should remain 1 (don't fragment graph)
   - > 1 indicates over-sparsification

7. Runtime:
   - Sparsification time
   - Community detection time
   - Total pipeline time
   - Speedup = baseline_time / sparse_time

8. G-term Change:
   - G^(s) - G^(1)
   - Should be ≤ 0 for improvement
   - Measure community degree variance


# ===========================================================================
# STATISTICAL VALIDATION
# ===========================================================================


For each claim in the paper:

CLAIM 1: "Modularity gains of 3-33% at 50% retention"
→ Evidence: Experiment 2.1, aggregate across datasets
→ Report: Mean ± std, effect size (Cohen's d)
→ Test: One-sample t-test vs 0

CLAIM 2: "NMI improvements up to 12%"
→ Evidence: Experiment 2.1 on networks with ground truth
→ Report: Maximum observed, conditions where it occurs
→ Test: Paired t-test vs baseline

CLAIM 3: "Hub-bridge hypothesis: Inter-edges connect higher-degree nodes"
→ Evidence: Experiments 1.1, 1.3, 2.1
→ Report: Correlation, KS test p-values
→ Test: Wilcoxon signed-rank test (degree products inter vs intra)

CLAIM 4: "Theorem 1 correctly predicts preservation ratio"
→ Evidence: Experiments 1.2
→ Report: Pearson r between predicted and observed
→ Test: r > 0.95 threshold, RMSE

CLAIM 5: "2.8× speedup"
→ Evidence: Experiment 3.1
→ Report: Speedup at 50% retention, scaling analysis
→ Test: Regression on log(time) vs log(m), slope ≈ 1

CLAIM 6: "LFR doesn't capture phenomenon"
→ Evidence: Experiment 1.3
→ Report: δ and hub-bridge correlation in LFR vs real
→ Test: t-test comparing standard vs modified LFR

CLAIM 7: "Boundary conditions: Fails on homogeneous networks"
→ Evidence: Experiment 4.2
→ Report: δ ≈ 0, no improvement on lattices/regular graphs
→ Test: Compare improvement on homogeneous vs heterogeneous


# ===========================================================================
# FIGURE GENERATION GUIDE
# ===========================================================================


RECOMMENDED FIGURES FOR PAPER:

Figure 1: Mechanism Proof (Experiment 1.1)
  Layout: 2×2 grid showing factorial design
  Panels: uniform/uniform, uniform/powerlaw, powerlaw/uniform, powerlaw/powerlaw
  Content: Bar plot of modularity change
  Message: Only powerlaw+preferential shows improvement

Figure 2: Theoretical Validation (Experiment 1.2)
  Layout: 3 panels (A, B, C)
  Panel A: Scatter plot - predicted vs observed ratio (y=x line)
  Panel B: Bar plot - δ for each dataset
  Panel C: Line plot - Modularity vs retention (multiple methods)
  Message: Theory quantitatively predicts outcomes

Figure 3: Real Network Results (Experiment 2.1)
  Layout: 6 panels (2×3 grid)
  Content: Modularity vs retention for 6 diverse networks
  Lines: DSpar, Random, Degree-threshold, Baseline
  Message: DSpar consistently outperforms baselines

Figure 4: LFR Analysis (Experiment 1.3)
  Layout: 2 panels
  Panel A: Histogram - degree product distributions (inter vs intra)
            Show: Standard LFR (no difference), Real network (clear difference)
  Panel B: Bar plot - δ comparison (Standard LFR vs Modified LFR vs Real)
  Message: LFR's uniform inter-edge placement is the problem

Figure 5: Scalability (Experiment 3.1)
  Layout: 2 panels
  Panel A: Log-log plot - Time vs edges (with O(m) reference line)
  Panel B: Bar plot - Speedup vs network size
  Message: O(m) scaling confirmed, practical speedup achieved

Figure 6: Diagnostic Framework (Experiment 4.1)
  Layout: Single scatter plot
  X-axis: δ (DSpar separation)
  Y-axis: Modularity improvement (%)
  Points: Colored by network type
  Annotation: Decision boundary (δ > 0)
  Message: δ predicts when DSpar works


# ===========================================================================
# PAPER SECTIONS MAPPING
# ===========================================================================


EXPERIMENTS → PAPER SECTIONS:

Section IV: EXPERIMENTS
  A. Datasets
     → Experiments 2.1, 4.1 (network statistics table)
  
  B. Mechanism Validation
     → Experiment 1.1 (factorial design)
     → Experiment 1.2 (quantitative predictions)
     → Experiment 1.3 (LFR analysis)
     → Figure 1, 2, 4
  
  C. Real Network Performance
     → Experiment 2.1 (main results)
     → Experiment 2.2 (weighting vs sampling)
     → Figure 3
  
  D. Efficiency Analysis
     → Experiment 3.1 (scalability)
     → Figure 5
  
  E. Boundary Conditions
     → Experiment 4.1 (diagnostic framework)
     → Experiment 4.2 (failure cases)
     → Figure 6

Section V: DISCUSSION
  → Synthesis of 1.1, 1.3 (why mechanism matters)
  → Practical implications from 2.1, 3.1
  → Limitations from 4.2





# ===========================================================================
# DATA AVAILABILITY & REPRODUCIBILITY
# ===========================================================================


DATASETS USED:

Citation Networks:
  - cit-HepPh: ArXiv HEP-PH (28k nodes, 4.6M edges)
  - cit-HepTh: ArXiv HEP-TH (27k nodes, 352k edges)
  Source: SNAP (http://snap.stanford.edu/data/)

Social Networks:
  - facebook: Facebook combined (4k nodes, 88k edges)
  Source: SNAP

Collaboration Networks:
  - ca-GrQc: General Relativity collaboration (5k nodes, 14k edges)
  Source: SNAP

Communication Networks:
  - email-Enron: Enron email network (36k nodes, 183k edges)
  Source: SNAP

Boundary Cases:
  - road-CA: California road network (1.9M nodes, 2.7M edges)
  Source: SNAP

com-DBLP
com-Youtube
email-Eu-core
Most of the datasets can be found at datasets/ and be loaded by utils.py


